# ğŸ“š Projeto Inspira - Web Crawler

## DescriÃ§Ã£o

Este projeto Ã© um Web Crawler desenvolvido com **Scrapy** para coletar informaÃ§Ãµes de livros do site [Books to Scrape](http://books.toscrape.com/). Os dados extraÃ­dos sÃ£o armazenados em um arquivo **JSON** e posteriormente exibidos em uma interface interativa criada com **Streamlit**.

Web scraping, tambÃ©m conhecido como web crawling, Ã© uma ferramenta eficiente para coletar dados da web [1]. Com um web scraper, Ã© possÃ­vel extrair dados sobre produtos, grandes volumes de texto, dados quantitativos ou dados de sites sem uma API oficial [1]. O Scrapy framework Ã© uma ferramenta poderosa para criar web scrapers em Python [2][4][5].

## ğŸ›  Tecnologias Utilizadas

- [**Python**](https://www.python.org/)
- [**Scrapy**](https://scrapy.org/) - Para realizar o web scraping [2][4][5]
- [**Streamlit**](https://streamlit.io/) - Para criar a interface web interativa
- [**Pandas**](https://pandas.pydata.org/) - Para manipulaÃ§Ã£o e anÃ¡lise dos dados coletados

## ğŸ“‚ Estrutura do Projeto

INSPIRA-TEST/
â”‚-- scrapy_crawler/
â”‚ â”œâ”€â”€ scrapy_crawler/
â”‚ â”‚ â”œâ”€â”€ spiders/
â”‚ â”‚ â”‚ â”œâ”€â”€ crawling_spider.py # Spider para coletar dados do site
â”‚ â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ books.json # Arquivo gerado com os dados coletados
â”‚ â”‚ â”œâ”€â”€ items.py
â”‚ â”‚ â”œâ”€â”€ middlewares.py
â”‚ â”‚ â”œâ”€â”€ pipelines.py
â”‚ â”‚ â”œâ”€â”€ settings.py
â”‚ â”‚ â”œâ”€â”€ scrapy.cfg
â”‚-- venv/ # Ambiente virtual do Python
â”‚-- requirements.txt # DependÃªncias do projeto
â”‚-- streamlit_app.py # Interface do Streamlit para visualizaÃ§Ã£o dos dados


## ğŸš€ Como Executar o Projeto

1.  **Configurar o ambiente virtual**

    ```
    python -m venv venv
    source venv/bin/activate  # Linux/macOS
    venv\Scripts\activate  # Windows
    ```
2.  **Instalar dependÃªncias**

    ```
    pip install -r requirements.txt
    ```
3.  **Executar o Web Crawler**

    ```
    cd scrapy_crawler
    scrapy crawl booksscraper -o scrapy_crawler/books.json
    ```

    Isso irÃ¡ coletar os dados do site e armazenÃ¡-los no arquivo `books.json` [2].
4.  **Rodar a Interface no Streamlit**

    ```
    streamlit run streamlit_app.py
    ```

    Isso abrirÃ¡ a interface no navegador, permitindo visualizar e interagir com os dados coletados.

## ğŸ¯ Funcionalidades

-   **Coleta de dados de livros**: ExtraÃ§Ã£o de tÃ­tulo, preÃ§o, disponibilidade, gÃªnero e avaliaÃ§Ã£o.
-   **Interface interativa**: ExibiÃ§Ã£o dos dados coletados com filtros e mÃ©tricas.
-   **CÃ¡lculo de preÃ§o mÃ©dio**: Exibe o preÃ§o mÃ©dio geral e por gÃªnero.
-   **DistribuiÃ§Ã£o de avaliaÃ§Ãµes**: Apresenta a quantidade de livros por nÃºmero de estrelas.

## ğŸ“Œ ObservaÃ§Ãµes

-   O projeto roda em um ambiente virtual para evitar conflitos de dependÃªncias.
-   A interface sÃ³ funcionarÃ¡ corretamente se houver um `books.json` gerado pelo Scrapy.
